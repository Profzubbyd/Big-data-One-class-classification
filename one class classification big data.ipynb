{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc39b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer   \n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.serializers import PickleSerializer\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import random\n",
    "from pyswarm import pso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50440675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/24 17:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Spark context object using the configuration\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").config(\"spark.driver.memory\", \"28g\").config(\"spark.executor.memory\", \"80g\").config(\"spark.driver.maxResultSize\", \"10g\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014bec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_rdd = spark.sparkContext.textFile(\"file:///gpfs/SHPC_Data/home/aafolab4/ccFraud.csv\");\n",
    "filtered_rdd = lines_rdd.filter(lambda x: x != '\"custID\",\"gender\",\"state\",\"cardholder\",\"balance\",\"numTrans\",\"numIntlTrans\",\"creditLine\",\"fraudRisk\"')\n",
    "filtered_rdd = filtered_rdd.map(lambda x: (x))\n",
    "parsed_rdd = filtered_rdd.map(lambda line: line.split(\",\"))\n",
    "parsed_rdd = parsed_rdd.map(lambda x : tuple(x))\n",
    "parsed_rdd = parsed_rdd.map(lambda x : tuple(np.float32(x)))\n",
    "data_rdd = parsed_rdd.map(lambda x : (int(x[-1]),x[1:8]))\n",
    "#train_rdd = data_rdd.filter(lambda x : x[0] == 0).map(lambda x: x[1])\n",
    "#train_labels_rdd =  data_rdd.filter(lambda x : x[0] == 0).map(lambda x: x[0])\n",
    "#test_rdd = data_rdd.filter(lambda x : x[0] == 1).map(lambda x: x[1])\n",
    "#test_labels_rdd = data_rdd.filter(lambda x : x[0] == 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1909b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0, 35.0, 1.0, 3000.0, 4.0, 14.0, 2.0, 0.0),\n",
       " (2.0, 2.0, 2.0, 1.0, 0.0, 9.0, 0.0, 18.0, 0.0),\n",
       " (3.0, 2.0, 2.0, 1.0, 0.0, 27.0, 9.0, 16.0, 0.0),\n",
       " (4.0, 1.0, 15.0, 1.0, 0.0, 12.0, 0.0, 5.0, 0.0),\n",
       " (5.0, 1.0, 46.0, 1.0, 0.0, 11.0, 16.0, 7.0, 0.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbd5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = data_rdd.filter(lambda x : x[0] == 0).map(lambda x: x[1])\n",
    "train_labels_rdd =  data_rdd.filter(lambda x : x[0] == 0).map(lambda x: x[0])\n",
    "test_rdd = data_rdd.filter(lambda x : x[0] == 1).map(lambda x: x[1])\n",
    "test_labels_rdd = data_rdd.filter(lambda x : x[0] == 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ef56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "a= train_rdd.collect()\n",
    "a = np.array(a)\n",
    "b= np.array(test_rdd.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f01f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSOAANN:\n",
    "    def __init__(self, n_hidden, c1, c2, w):\n",
    "        self.n_hidden = n_hidden\n",
    "       # self.scaler = None\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.w = w\n",
    "        \n",
    "   \n",
    "    def normalize(self, X):\n",
    "        X_rdd = spark.sparkContext.parallelize(X, numSlices=10000)\n",
    "        min_vals = X_rdd.reduce(lambda x, y: tuple(map(min, zip(x, y))))\n",
    "        max_vals = X_rdd.reduce(lambda x, y: tuple(map(max, zip(x, y))))\n",
    "        X_scaled_rdd = X_rdd.map(lambda x : tuple((np.array(x) - np.array(min_vals)) / (np.array(max_vals) - np.array(min_vals))))\n",
    "        X_scaled = np.array(X_scaled_rdd.collect())\n",
    "        return X_scaled\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        X_scaled = self.normalize(X)\n",
    "        \n",
    "        n_input = X_scaled.shape[1]\n",
    "        #n_output = len(np.unique(y))\n",
    "        \n",
    "        # Define bounds for PSO optimization\n",
    "        lower_bounds = [-5.0] * (n_input * self.n_hidden + self.n_hidden * n_input)\n",
    "        upper_bounds = [5.0] * (n_input * self.n_hidden + self.n_hidden * n_input)\n",
    "        bounds = (lower_bounds, upper_bounds)\n",
    "        \n",
    "        \n",
    "        # Define PSO fitness function\n",
    "        def fitness(params):\n",
    "            w1 = params[:n_input * self.n_hidden].reshape((n_input, self.n_hidden))\n",
    "            w2 = params[n_input * self.n_hidden:].reshape((self.n_hidden, n_input))\n",
    "            \n",
    "            # Feedforward calculation\n",
    "            a1 = np.dot(X_scaled, w1)\n",
    "            a1_rdd = spark.sparkContext.parallelize(a1, numSlices=10000)\n",
    "            a1_rdd = a1_rdd.map(lambda x: tuple(map(lambda y: 1.0 / (1.0 + np.exp(-y)), x)))\n",
    "            a1 = np.array(a1_rdd.collect())\n",
    "            \n",
    "            X_pred = np.dot(a1, w2)\n",
    "            X_pred_rdd = spark.sparkContext.parallelize(X_pred, numSlices=10000)\n",
    "            X_pred_rdd = X_pred_rdd.map(lambda x: tuple(map(lambda y: 1.0 / (1.0 + np.exp(-y)), x)))\n",
    "            X_pred = np.array(X_pred_rdd.collect())\n",
    "            \n",
    "            # Calculate accuracy as fitness score\n",
    "            #mse = np.sum((X_scaled-X_pred)**2)/(self.n_hidden * n_input)\n",
    "            mse = np.mean((X_scaled-X_pred)**2)\n",
    "            #print(mse)\n",
    "            #y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "            #score = accuracy_score(y, y_pred_labels)\n",
    "            return mse\n",
    "  \n",
    "        # Run PSO optimization\n",
    "        mse_mean = np.zeros(1)\n",
    "        params = np.random.uniform(low=-5,high=5,size=n_input * self.n_hidden * 2)\n",
    "        for i in range(1):\n",
    "            params, _ = pso(fitness, *bounds, swarmsize=1, maxiter=1, omega=self.w, phip=self.c1, phig=self.c2)\n",
    "            mse = fitness(params)\n",
    "            mse_mean[i] = np.mean(np.array(mse))\n",
    "            print(\"Iteration {} gives MSE {}\".format(i, mse)) \n",
    "\n",
    "             # Retrieve best weights\n",
    "            self.w1 = params[:n_input * self.n_hidden].reshape((n_input, self.n_hidden))\n",
    "            self.w2 = params[n_input * self.n_hidden:].reshape((self.n_hidden, n_input))\n",
    "        print(\"The final mean mse is \"+str(np.mean(mse_mean)))\n",
    "    def predict(self, X):\n",
    "        X_scaled = self.normalize(X)\n",
    "        a1 = np.dot(X_scaled, self.w1)\n",
    "        a1_rdd = spark.sparkContext.parallelize(a1, numSlices=10000)\n",
    "        a1_rdd = a1_rdd.map(lambda x: tuple(map(lambda y: 1.0 / (1.0 + np.exp(-y)), x)))\n",
    "        a1 = np.array(a1_rdd.collect())\n",
    "\n",
    "        X_pred = np.dot(a1, self.w2)\n",
    "        X_pred_rdd = spark.sparkContext.parallelize(X_pred, numSlices=10000)\n",
    "        X_pred_rdd = X_pred_rdd.map(lambda x: tuple(map(lambda y: 1.0 / (1.0 + np.exp(-y)), x)))\n",
    "        X_pred = np.array(X_pred_rdd.collect())\n",
    "\n",
    "        #relative error\n",
    "        relative_error = np.true_divide(np.abs(X_pred - X_scaled),X_scaled)\n",
    "        #y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        return relative_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8adc024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========>                                        (1961 + 32) / 10000]23/04/24 17:20:40 WARN TaskSetManager: Stage 3 contains a task of very large size (157 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 4:=========>                                         (1924 + 32) / 10000]23/04/24 17:20:55 WARN TaskSetManager: Stage 4 contains a task of very large size (157 KB). The maximum recommended task size is 100 KB.\n",
      "[Stage 5:==========>                                        (1967 + 32) / 10000]23/04/24 17:21:11 WARN TaskSetManager: Stage 5 contains a task of very large size (157 KB). The maximum recommended task size is 100 KB.\n",
      "23/04/24 17:22:09 WARN TaskSetManager: Stage 6 contains a task of very large size (100 KB). The maximum recommended task size is 100 KB.\n",
      "23/04/24 17:23:13 WARN TaskSetManager: Stage 7 contains a task of very large size (108 KB). The maximum recommended task size is 100 KB.\n",
      "23/04/24 17:24:22 WARN TaskSetManager: Stage 8 contains a task of very large size (100 KB). The maximum recommended task size is 100 KB.\n",
      "23/04/24 17:25:27 WARN TaskSetManager: Stage 9 contains a task of very large size (108 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping search: maximum iterations reached --> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/24 17:26:36 WARN TaskSetManager: Stage 10 contains a task of very large size (100 KB). The maximum recommended task size is 100 KB.\n",
      "23/04/24 17:27:40 WARN TaskSetManager: Stage 11 contains a task of very large size (108 KB). The maximum recommended task size is 100 KB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 gives MSE 0.2152761360896253\n",
      "The final mean mse is 0.2152761360896253\n",
      "Training this took  493.803  secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/SHPC_Data/home/aafolab4/.conda/envs/myJupyter/lib/python3.7/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification rate is 0.44537544420097513\n"
     ]
    }
   ],
   "source": [
    "psoaann = PSOAANN(n_hidden=6, c1=2, c2=2, w=0.9)\n",
    "start = time.time()\n",
    "psoaann.fit(a)\n",
    "finish = time.time()\n",
    "print(\"Training this took \", round(finish-start,3), \" secs\")\n",
    "X_pred = psoaann.predict(b)\n",
    "def final_pred(X_pred,threshold):\n",
    "    h = np.where(X_pred > threshold ,1, 0)\n",
    "    pred_class = []\n",
    "    for i in h:\n",
    "        if list(i).count(1) == len( list(i)) :\n",
    "            pred_class.append(1)\n",
    "        else:\n",
    "            pred_class.append(0)\n",
    "    \n",
    "    return pred_class\n",
    "\n",
    "pred = np.array(final_pred(b,0.05))\n",
    "\n",
    "Classification_rate = np.count_nonzero(pred) /len(b)\n",
    "print(\"The classification rate is \"+str(Classification_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cd230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059cb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd320d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
